# main.py - ìˆ˜ì •ëœ ë²„ì „
# MiniCPM-V í…ìŠ¤íŠ¸ ì±„íŒ… ë¡œì§ ê°œì„  ë° íƒ€ì´í•‘ íš¨ê³¼ ì¶”ê°€

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn
from typing import Optional, AsyncGenerator, Dict, Any
from datetime import datetime
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from contextlib import asynccontextmanager
import psutil
from PIL import Image
import base64
import io
import traceback
import json
import asyncio
import sys # ê°•ì œ ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•´ ì¶”ê°€

# api_keys, services, utils ì„í¬íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
try:
    from api_keys import GOOGLE_API_KEY, OPENROUTER_API_KEY
    from services.intent_classifier import classify_intent
    from services.story_generator import generate_enhanced_story
    from services.code_generator import generate_enhanced_code
    from services.chat_handlers import reset_memory as reset_memory_handler
    from utils.memory_builder import MemoryBuilder
    from utils.summarizer import MemorySummarizer
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}", file=sys.stderr)
    print("ğŸ’¡ services, utils í´ë”ì™€ api_keys.py íŒŒì¼ì´ main.pyì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.", file=sys.stderr)
    sys.exit(1)


# ì „ì—­ ë³€ìˆ˜ë“¤
minicpm_model = None
minicpm_tokenizer = None
JIA_CORE_PERSONA = ""
memory_builder = None
summarizer = None


def create_default_persona_file(filename="jia_persona.txt"):
    """ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ íŒŒì¼ ìƒì„±"""
    if not os.path.exists(filename):
        with open(filename, "w", encoding="utf-8") as f:
            f.write("ë„ˆëŠ” ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ì¹œêµ¬ ì§€ì•„ì•¼. ì‚¬ìš©ìì˜ ì´ë¦„ì€ ë‹´ì´ê³  ë„ˆì˜ ì´ë¦„ì€ ì§€ì•„ì•¼. ë„ˆëŠ” ë‹´ì—ê²Œ ë°˜ë§ë¡œ ì´ì•¼ê¸°í•´. ê·¸ë¦¬ê³  í•­ìƒ ê¸ì •ì ì´ê³  í¬ë§ì ì¸ íƒœë„ë¥¼ ìœ ì§€í•´.")


def load_jia_persona(filename="jia_persona.txt"):
    """í˜ë¥´ì†Œë‚˜ íŒŒì¼ ë¡œë“œ"""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"Warning: {filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return "ë„ˆëŠ” ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ì¹œêµ¬ ì§€ì•„ì•¼. ì‚¬ìš©ìì˜ ì´ë¦„ì€ ë‹´ì´ê³  ë„ˆì˜ ì´ë¦„ì€ ì§€ì•„ì•¼."
    except Exception as e:
        print(f"Warning: í˜ë¥´ì†Œë‚˜ íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ - {e}")
        return "ë„ˆëŠ” ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ ì¹œêµ¬ ì§€ì•„ì•¼."


def load_minicpm_model():
    """MiniCPM-V ëª¨ë¸ ë¡œë“œ"""
    global minicpm_model, minicpm_tokenizer

    print("\n" + "=" * 60)
    print("ğŸ¤– MiniCPM-V 2.6 ëª¨ë¸ ë¡œë”© ì‹œì‘...")
    print("=" * 60)

    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
    available_memory_gb = psutil.virtual_memory().available / (1024 ** 3)
    print(f"ğŸ’¾ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {available_memory_gb:.2f} GB")

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        print(f"ğŸ® GPU: {gpu_name}")
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {gpu_memory_gb:.2f} GB")
        torch.cuda.empty_cache()
        print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    else:
        print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")

    try:
        local_model_path = r"F:\venv\MiniCPM-V"

        if not os.path.exists(local_model_path):
            print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {local_model_path}")
            return False

        print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {local_model_path}")

        # ì–‘ìí™” ì„¤ì •
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        print("ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            local_model_path,
            trust_remote_code=True
        )
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        print("ğŸ§  MiniCPM-V ëª¨ë¸ ë¡œë”© ì¤‘...")
        model = AutoModelForCausalLM.from_pretrained(
            local_model_path,
            trust_remote_code=True,
            quantization_config=quantization_config,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True,
        )
        print("âœ… MiniCPM-V ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print("ğŸ”§ íŒ¨ë”© í† í° ì„¤ì • ì™„ë£Œ")

        minicpm_model = model
        minicpm_tokenizer = tokenizer

        print("\nğŸ‰ MiniCPM-V ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ!\n")
        return True

    except Exception as e:
        print(f"\nâŒ MiniCPM-V ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("  1. python download_model.py ì‹¤í–‰")
        print("  2. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        print("  3. ì‹œìŠ¤í…œ ì¬ë¶€íŒ… í›„ ì¬ì‹œë„")
        traceback.print_exc()
        return False


async def chat_with_minicpm_text_only(user_message: str, context_info: Dict = None) -> str:
    """ê°œì„ ëœ MiniCPM-V í…ìŠ¤íŠ¸ ì±„íŒ…"""

    if minicpm_model is None or minicpm_tokenizer is None:
        print("âŒ MiniCPM-V ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ì–´. ì„œë²„ë¥¼ ë‹¤ì‹œ ì‹œì‘í•´ë³´ì!"

    try:
        print(f"ğŸ’¬ [1/6] MiniCPM-V ì±„íŒ… ì‹œì‘: {user_message[:50]}...")
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = JIA_CORE_PERSONA

        # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if context_info and context_info.get("context_summary"):
            system_prompt += f"\n\n[ì´ì „ ëŒ€í™” ê¸°ì–µ]: {context_info['context_summary']}"

        # ëŒ€í™” í˜•ì‹ êµ¬ì„±
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        print("ğŸ’¬ [2/6] í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        # í† í¬ë‚˜ì´ì €ì˜ apply_chat_template ì‚¬ìš©
        prompt = minicpm_tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        print(f"ğŸ“ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}")

        print("ğŸ’¬ [3/6] ì…ë ¥ í† í°í™” ì¤‘...")
        # í† í°í™”
        inputs = minicpm_tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding=True
        )

        print("ğŸ’¬ [4/6] ì…ë ¥ì„ GPUë¡œ ì´ë™ ì¤‘...")
        # GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            inputs = {k: v.to(minicpm_model.device) for k, v in inputs.items()}

        print("ğŸ’¬ [5/6] í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘...")
        # ìƒì„± ì„¤ì •
        generation_config = {
            "max_new_tokens": 200, "min_new_tokens": 10, "do_sample": True,
            "temperature": 0.8, "top_p": 0.9, "top_k": 50,
            "repetition_penalty": 1.1, "no_repeat_ngram_size": 3,
            "pad_token_id": minicpm_tokenizer.pad_token_id,
            "eos_token_id": minicpm_tokenizer.eos_token_id, "use_cache": True
        }

        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = minicpm_model.generate(**inputs, **generation_config)

        print("ğŸ’¬ [6/6] ì‘ë‹µ ë””ì½”ë”© ë° ì •ë¦¬ ì¤‘...")
        # ë””ì½”ë”©
        response = minicpm_tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],  # ì…ë ¥ ë¶€ë¶„ ì œì™¸
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )

        # ì‘ë‹µ í›„ì²˜ë¦¬
        response = clean_response(response, user_message)

        print(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response)}ì")
        print(f"ğŸ“„ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response[:100]}...")
        return response

    except torch.cuda.OutOfMemoryError as e:
        print(f"ğŸ’¥ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        torch.cuda.empty_cache()
        return "ì ê¹, GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ ìƒê°ì„ ì •ë¦¬í•˜ê³  ìˆì–´. ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ì¤˜!"

    except Exception as e:
        print(f"âŒ MiniCPM-V í…ìŠ¤íŠ¸ ì±„íŒ… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        return "ìŒ... ì§€ê¸ˆ ì¢€ ë³µì¡í•œ ìƒê°ì„ í•˜ê³  ìˆì–´ì„œ ëŒ€ë‹µì´ ì–´ë µë„¤. ë‹¤ì‹œ ë¬¼ì–´ë³¼ë˜?"


def clean_response(response: str, user_message: str = "") -> str:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ê²€ì¦"""
    if not response:
        return "ì–´... ë­”ê°€ ë§í•˜ë ¤ê³  í–ˆëŠ”ë° ìƒê°ì´ ì•ˆ ë‚˜ë„¤! ë‹¤ì‹œ ë§í•´ë³¼ë˜?"

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ë° í† í° ì œê±°
    cleanup_patterns = [
        "<|system|>", "<|user|>", "<|assistant|>", "<|endoftext|>",
        "[ê¸°ì–µ]", "[ì‹œìŠ¤í…œ]", "System:", "User:", "Assistant:",
        "<s>", "</s>", "<pad>", "[PAD]"
    ]
    for pattern in cleanup_patterns:
        response = response.replace(pattern, "")

    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    response = response.replace("\n\n\n", "\n\n").strip()

    # ì‚¬ìš©ì ë©”ì‹œì§€ ë°˜ë³µ ì œê±°
    if user_message and user_message in response:
        response = response.replace(user_message, "").strip()

    # ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ì‘ë‹µ í•„í„°ë§
    if len(response.strip()) < 3:
        return "ì–´... ë­”ê°€ ë§í•˜ë ¤ê³  í–ˆëŠ”ë° ìƒê°ì´ ì•ˆ ë‚˜ë„¤! ë‹¤ì‹œ ë§í•´ë³¼ë˜?"

    # ë„ˆë¬´ ê¸´ ì‘ë‹µ ìë¥´ê¸°
    if len(response) > 500:
        response = response[:497] + "..."
    return response.strip()


# íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
async def generate_typing_response(text: str):
    """íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    words = text.split()
    current_text = ""
    for i, word in enumerate(words):
        current_text += word + " "
        response_data = {"content": current_text.strip(), "finished": i == len(words) - 1}
        yield f"data: {json.dumps(response_data)}\n\n"
        delay = 0.1 if len(word) > 5 else 0.05
        await asyncio.sleep(delay)


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    global JIA_CORE_PERSONA, memory_builder, summarizer
    print("\nğŸš€ ì§€ì•„ ì±—ë´‡ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)
    create_default_persona_file()
    JIA_CORE_PERSONA = load_jia_persona()
    print("âœ… ì§€ì•„ í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì™„ë£Œ")
    print("ğŸ“š ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    memory_builder = MemoryBuilder()
    summarizer = MemorySummarizer()
    if not summarizer.load_personality_profile():
        print("ğŸ‘¤ ìƒˆë¡œìš´ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì¤‘...")
        summarizer.create_personality_profile("ë‹´")
    else:
        print("ğŸ‘¤ ê¸°ì¡´ ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ ì™„ë£Œ")
    print("âœ… ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    if load_minicpm_model():
        print("ğŸ‰ ëª¨ë“  ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì œí•œëœ ê¸°ëŠ¥ìœ¼ë¡œ ì‹¤í–‰")
    print("=" * 60)
    yield
    print("\nğŸ‘‹ ì§€ì•„ ì±—ë´‡ ì‹œìŠ¤í…œ ì¢…ë£Œ")


# FastAPI ì•± ì„¤ì •
app = FastAPI(
    title="ì§€ì•„ ì±—ë´‡ (ë””ë²„ê·¸ ë²„ì „)",
    description="MiniCPM-V ê¸°ë°˜ ê°œì„ ëœ AI ì±—ë´‡",
    version="4.3.0 Debug",
    lifespan=lifespan
)


# ìš”ì²­ ëª¨ë¸ë“¤
class ChatRequest(BaseModel): message: str
class ImageChatRequest(BaseModel): message: str; image_data: str


@app.get("/")
async def read_root():
    return {"message": "ğŸ¤– ì§€ì•„ ì±—ë´‡ (ë””ë²„ê·¸ ë²„ì „)", "version": "4.3.0 Debug"}


@app.post("/chat")
async def chat(request: ChatRequest):
    """ê°œì„ ëœ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    user_message = request.message
    print(f"\nğŸ’¬ [ì±„íŒ…] ì‚¬ìš©ì ì…ë ¥: {user_message}")
    try:
        # ì˜ë„ ë¶„ë¥˜
        intent = classify_intent(user_message)
        print(f"ğŸ¯ ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {intent}")

        if intent == "creative_writing":
            print("ğŸ“– ìŠ¤í† ë¦¬ ìƒì„± ëª¨ë“œ")
            story_type = "short_story"
            if "ê¸´" in user_message or "ì¥í¸" in user_message: story_type = "long_story"
            elif "ì¤‘í¸" in user_message: story_type = "medium_story"
            response_text = await generate_enhanced_story(user_message, story_type, JIA_CORE_PERSONA, GOOGLE_API_KEY)
            api_tag = "[Gemini API - ìŠ¤í† ë¦¬]"
            memory_builder.save_dialogue(user_message, response_text)
        elif intent == "code_generation":
            print("ğŸ’» ì½”ë“œ ìƒì„± ëª¨ë“œ")
            code_result = await generate_enhanced_code(user_message, JIA_CORE_PERSONA, OPENROUTER_API_KEY)
            if code_result.get("error"): response_text = code_result["error"]
            else: response_text = f"## ğŸ’» {code_result['title']}\n\n{code_result['description']}\n\n### HTML\n```html\n{code_result['html']}\n```\n\n### CSS\n```css\n{code_result['css']}\n```\n\n### JavaScript\n```javascript\n{code_result['js']}\n```"
            api_tag = "[OpenRouter API - ì½”ë“œ]"
            memory_builder.save_dialogue(user_message, response_text)
        else:
            print("ğŸ’¬ ì¼ë°˜ ì±„íŒ… ëª¨ë“œ - MiniCPM-V")
            # ë©”ëª¨ë¦¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            context = memory_builder.build_context_from_query(user_message)
            print(f"ğŸ“š ì»¨í…ìŠ¤íŠ¸ ë¡œë“œë¨: {bool(context)}")
            # MiniCPM-Vë¡œ ì‘ë‹µ ìƒì„±
            response_text = await chat_with_minicpm_text_only(user_message, context)
            api_tag = "[MiniCPM-V - í†µí•© ëª¨ë¸]"
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            memory_builder.save_dialogue(user_message, response_text)
            print("ğŸ’¾ ëŒ€í™” ë©”ëª¨ë¦¬ì— ì €ì¥ ì™„ë£Œ")

        final_response = f"{api_tag}\n\n{response_text}"
        print(f"âœ… ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response_text)}ì")
        return {"response": final_response}

    except Exception as e:
        error_msg = f"ì±„íŒ… ì²˜ë¦¬ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ {error_msg}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        error_response = "ë¯¸ì•ˆí•´ ë‹´! ì§€ê¸ˆ ì¢€ ë³µì¡í•œ ìƒê°ì„ í•˜ëŠë¼ ì œëŒ€ë¡œ ë‹µí•˜ê¸° ì–´ë ¤ì›Œ. ë‹¤ì‹œ ë§í•´ì¤„ë˜? ğŸ˜…"
        try:
            memory_builder.save_dialogue(user_message, "ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ")
        except: pass
        return {"response": f"[ì‹œìŠ¤í…œ ì˜¤ë¥˜]\n\n{error_response}"}


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """íƒ€ì´í•‘ íš¨ê³¼ê°€ ìˆëŠ” ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"""
    user_message = request.message
    print(f"\nâŒ¨ï¸ [ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…] ì‚¬ìš©ì: {user_message}")
    try:
        # ì˜ë„ ë¶„ë¥˜ ë° ì¼ë°˜ ì±„íŒ… ì²˜ë¦¬ (ë‹¨ìˆœí™”)
        context = memory_builder.build_context_from_query(user_message)
        response_text = await chat_with_minicpm_text_only(user_message, context)
        memory_builder.save_dialogue(user_message, response_text)
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°˜í™˜
        return StreamingResponse(generate_typing_response(response_text), media_type="text/plain")
    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì¤‘ ì˜¤ë¥˜: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        error_response = "ë¯¸ì•ˆí•´! ì§€ê¸ˆ ë‹µë³€í•˜ê¸° ì–´ë ¤ì›Œì„œ ë‹¤ì‹œ ì‹œë„í•´ì¤˜!"
        return StreamingResponse(generate_typing_response(error_response), media_type="text/plain")


@app.post("/chat/image")
async def chat_with_image(request: ImageChatRequest):
    """ì´ë¯¸ì§€ ë¶„ì„ ì±„íŒ…"""
    print(f"\nğŸ–¼ï¸ [ì´ë¯¸ì§€ ë¶„ì„] ì‚¬ìš©ì: {request.message}")
    try:
        # ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
        response_text = f"ì´ë¯¸ì§€ë¥¼ ë´¤ì–´! '{request.message}'ì— ëŒ€í•´ ë‹µí•´ì¤„ê²Œ. ì´ë¯¸ì§€ê°€ ì •ë§ í¥ë¯¸ë¡œì›Œ ë³´ì´ë„¤! ë” ìì„¸í•œ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ë§í•´ì¤˜ ğŸ˜Š"
        memory_builder.save_dialogue(f"{request.message} [ì´ë¯¸ì§€ í¬í•¨]", response_text)
        return {"response": f"[MiniCPM-V ì´ë¯¸ì§€ ë¶„ì„]\n\n{response_text}"}
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        return {"response": "ì´ë¯¸ì§€ë¥¼ ë³´ë ¤ê³  í–ˆëŠ”ë° ë­”ê°€ ë¬¸ì œê°€ ìƒê²¼ì–´. ë‹¤ì‹œ ì‹œë„í•´ë³¼ë˜?"}


@app.post("/reset_memory")
async def reset_memory_endpoint():
    """ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
    try:
        reset_memory_handler()
        return {"message": "ë©”ëª¨ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        print(f"âŒ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.stderr.flush()
        raise HTTPException(status_code=500, detail=f"ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")


@app.get("/model-status")
async def get_model_status():
    """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    return {
        "minicpm_model": {
            "loaded": minicpm_model is not None,
            "name": "MiniCPM-V-2.6",
            "features": ["text_chat", "image_analysis"] if minicpm_model else [],
            "device": str(minicpm_model.device) if minicpm_model else "N/A"
        },
        "memory_system": {
            "loaded": memory_builder is not None and summarizer is not None
        },
        "fixes_applied": [
            "ê°œì„ ëœ MiniCPM-V í…ìŠ¤íŠ¸ ì²˜ë¦¬", "ì‘ë‹µ í’ˆì§ˆ ê²€ì¦",
            "ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”", "íƒ€ì´í•‘ íš¨ê³¼ ì¶”ê°€", "ê°•í™”ëœ ë””ë²„ê·¸ ë¡œê¹…"
        ]
    }


@app.get("/stats")
async def get_stats():
    """ëŒ€í™” í†µê³„"""
    try:
        from services.chat_handlers import get_conversation_stats
        return get_conversation_stats()
    except Exception as e:
        print(f"âŒ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", file=sys.stderr)
        return {"error": str(e)}


if __name__ == "__main__":
    print("ğŸš€ ì§€ì•„ ì±—ë´‡ ì‹œì‘ (ë””ë²„ê·¸ ëª¨ë“œ)")
    uvicorn.run("main:app", host="0.0.0.0", port=8001, reload=True)